{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "203sCm8kMr6D"
      },
      "source": [
        "## Разработка модели машиного обучения для опознания вредоносных сайтов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRf7i7N49zco"
      },
      "source": [
        "### Загрузка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYRo7W-XnpSb"
      },
      "source": [
        "Загрузим необходимые библиотеки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KyN-rcRWjW1",
        "outputId": "4b5ced7a-7172-4ba6-d642-59f89174505a"
      },
      "outputs": [],
      "source": [
        "!pip install razdel -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYz-8lbPntZA"
      },
      "source": [
        "Импортируем необходимые библиотеки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "ZvZLXJRWMq8T",
        "outputId": "7d6c8b89-2401-403d-b9d0-41b8d470e805"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import razdel\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utbU36ZCnmzk"
      },
      "source": [
        "Загрузим датасеты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWvjCzmlMrIo"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('data/train.csv') \n",
        "df_test = pd.read_csv('data/test.csv') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMyQ-XK8p1fs"
      },
      "source": [
        "Проверим есть ли пропуски в тренировочном датасете"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BcuGnjYUp7Zm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество пропусков в тренировочных данных \n",
            "ID       0\n",
            "url      0\n",
            "title    1\n",
            "label    0\n",
            "dtype: int64\n",
            "\n",
            "Количество пропусков в тестовых данных \n",
            "ID       0\n",
            "url      0\n",
            "title    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(f'Количество пропусков в тренировочных данных \\n{df_train.isna().sum()}\\n')\n",
        "print(f'Количество пропусков в тестовых данных \\n{df_test.isna().sum()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d97clOBr6eT"
      },
      "source": [
        "В данных присутсвуют пропуски, обработаем их в пайплайне"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvTpLZMzr_4o"
      },
      "source": [
        "Разделим данные на тренировочную и валидационную выборки\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i9SvWczn9QP"
      },
      "source": [
        "Выделим зависимые и независимые переменные."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "a7vs4SQxMp2c"
      },
      "outputs": [],
      "source": [
        "X = df_train[['url','title']]\n",
        "y = df_train[['label']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPMb_dRyvNBV"
      },
      "source": [
        "Разделим данные на тренировочную и валидационную выборки\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "404C2R40vHru"
      },
      "outputs": [],
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42,stratify=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgitstOI5paT"
      },
      "source": [
        "### Модель: Обработка текста - токенизация и стемминг, логистическая регрессия"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUtCzmkSoHqM"
      },
      "source": [
        "Опишем класс трансформера в пайплайне, который принимает датасет с двумя строковыми столбцами и возвращеает одну объединённую строку."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XPJUYX9dUY4p"
      },
      "outputs": [],
      "source": [
        "class CombineColumns(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        title_column = X[:, 0]  # Первый столбец (title)\n",
        "        url_column = X[:, 1]    # Второй столбец (url)\n",
        "\n",
        "        # Объединяем столбцы 'title' и 'url' в одну строку\n",
        "        return (title_column + ' ' + url_column)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBkGRIl7okc3"
      },
      "source": [
        "Опишем класс трансформера в пайплайне, который будет принимать датасет состоящий из столбца со строками, удалять все стоп слова английского и русского языка, и производить стемматизацию"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hckeoLpfWuP0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words_r = set(stopwords.words(\"russian\"))\n",
        "stop_words_e = set(stopwords.words(\"english\"))\n",
        "\n",
        "class SnowballStemmerTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.stemmer = SnowballStemmer(\"russian\")\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        stemmed_texts = []\n",
        "        for text in X:\n",
        "            # Токенизируем текст с помощью ntkl\n",
        "            tokens = nltk.tokenize.casual_tokenize(text)\n",
        "\n",
        "            # Уберем русские и английские стоп слова\n",
        "            filtered_tokens = [token for token in tokens if token.lower() not in stop_words_r and token.lower() not in stop_words_e]\n",
        "\n",
        "            # Выполним стемминг с помощью SnowballStemmer\n",
        "            stemmed_words = [self.stemmer.stem(token) for token in filtered_tokens]\n",
        "\n",
        "            # Собираем слова обратно в строку\n",
        "            stemmed_text = ' '.join(stemmed_words)\n",
        "            stemmed_texts.append(stemmed_text)\n",
        "\n",
        "        return pd.Series(stemmed_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU54NeyZpL1Q"
      },
      "source": [
        "Опишем класс трансформера в пайплайне, который будет заменять пустые значения в датасете на пустые строки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9KG_5Wy0RVKZ"
      },
      "outputs": [],
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('title_url', SimpleImputer(strategy='constant', fill_value=''), ['title','url'])\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idVI0jJKpZ1p"
      },
      "source": [
        "Опишем сам пайплайн."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mmrRoVnrSofi"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),  # Применяем preprocessor для обработки пропусков\n",
        "    ('combine', CombineColumns()),  # Объединяем столбцы 'title' и 'url' в одну строку\n",
        "    ('stemmer', SnowballStemmerTransformer()),  # Стеммер для русского языка\n",
        "    ('vectorizer', TfidfVectorizer(stop_words='english', max_features=3000)),  # Векторизация текста\n",
        "    ('classifier', LogisticRegression())  # Классификатор\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE3P_xOfpdPz"
      },
      "source": [
        "Создадим метрику для поиска в gridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "m-FNJ787OXsB"
      },
      "outputs": [],
      "source": [
        "f1_scorer = make_scorer(f1_score, average='weighted')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfdweBVOqgWD"
      },
      "source": [
        "Опишем словарь параметров для поиска по сетке"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "y-uZDzbDFbxu"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    # Выбор векторизатора\n",
        "    'vectorizer': [TfidfVectorizer(max_features=8000, stop_words='english')],\n",
        "\n",
        "    # Параметры для LogisticRegression\n",
        "    'classifier__C': [25],  # Регуляризация\n",
        "    'classifier__penalty': ['l2'],\n",
        "    'classifier__solver': ['liblinear']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBoWrIikqmMJ"
      },
      "source": [
        "Произведем поиск по сетке и выведем модель с лучшими параметрами"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uHU2EytqQ96J"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Лучшие параметры: {'classifier__C': 25, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear', 'vectorizer': TfidfVectorizer(max_features=8000, stop_words='english')}\n",
            "Лучший результат по F1: 0.9918486893378257\n"
          ]
        }
      ],
      "source": [
        "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring=f1_scorer, cv=3)\n",
        "grid_search.fit(X_train, y_train.values.ravel())\n",
        "\n",
        "# Лучшие параметры и лучший результат\n",
        "print(\"Лучшие параметры:\", grid_search.best_params_)\n",
        "print(\"Лучший результат по F1:\", grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi5J6_VPqv5p"
      },
      "source": [
        "Сохраним лучшую модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LiNkn6KXqwZI"
      },
      "outputs": [],
      "source": [
        "best_model = grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FMHdOQXq2z4"
      },
      "source": [
        "Оценим метрику качества модели на валидационной выборке"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iUAMopP7RiEK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9650986342943855\n"
          ]
        }
      ],
      "source": [
        "y_pred = best_model.predict(X_valid)\n",
        "f1 = f1_score(y_valid, y_pred)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du9A1HVp6RoH"
      },
      "source": [
        "### Предсказание на тестовой выборке и сохранение результатов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBnGpCf2Q8Cy"
      },
      "source": [
        "Сделаем предсказание с помощью лучшей модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6Ql6YE5iQ98L"
      },
      "outputs": [],
      "source": [
        "test_predictions =  best_model.predict(df_test[['title', 'url']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQBvsogtrGLY"
      },
      "source": [
        "Сохраним предсказание в датафрейм"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "O78JQubwQ994"
      },
      "outputs": [],
      "source": [
        "submission = pd.DataFrame({\n",
        "    'ID': df_test['ID'],\n",
        "    'label': test_predictions\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxTkEyd9rKg1"
      },
      "source": [
        "Преобразуем в файл формата csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sjjPd_29Vhtq"
      },
      "outputs": [],
      "source": [
        "submission.to_csv('submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Вывод"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "В рамках данного проекта была разработана модель машинного обучения для классификации вредоносных и безопасных веб-сайтов на основе их URL-адресов и описания. Для преобразования текста в числовое представление использовались базовые методы обработки текстов (такие как TF-IDF и стемминг), а в качестве классификатора применялась логистическая регрессия.\n",
        "\n",
        "Модель была обучена на размеченном датасете и продемонстрировала приемлемые результаты на валидационном наборе. Ключевая метрика качества F1-score показала, что логистическая регрессия способна эффективно различать вредоносные и безопасные ссылки."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
